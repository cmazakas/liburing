/* SPDX-License-Identifier: MIT */
#ifndef LIB_URING_H
#define LIB_URING_H

#include <sys/socket.h>
#include <sys/stat.h>
#include <sys/uio.h>
#include <errno.h>
#include <signal.h>
#include <stdbool.h>
#include <inttypes.h>
#include <time.h>
#include <fcntl.h>
#include <sched.h>
#include <linux/swab.h>
#include <sys/wait.h>
#include "liburing/compat.h"
#include "liburing/io_uring.h"
#include "liburing/io_uring/query.h"
#include "liburing/io_uring_version.h"

#ifndef uring_unlikely
#define uring_unlikely(cond)	__builtin_expect(!!(cond), 0)
#endif

#ifndef uring_likely
#define uring_likely(cond)	__builtin_expect(!!(cond), 1)
#endif

/*
 * NOTE: Use IOURINGINLINE macro for "static inline" functions that are
 *       expected to be available in the FFI bindings. They must also
 *       be included in the liburing-ffi.map file.
 *
 *       Use _LOCAL_INLINE macro for "static inline" functions that are
 *       not expected to be available in the FFI bindings.
 *
 *       Don't use "static inline" directly when defining new functions
 *       in this header file.
 *
 *       Reason:
 *       The C++20 module export feature fails to operate correctly
 *       with the "static inline" functions. Use "inline" instead of
 *       "static inline" when compiling with C++20 or later.
 *
 *       See:
 *         https://github.com/axboe/liburing/issues/1457
 *         https://lore.kernel.org/io-uring/e0559c10-104d-4da8-9f7f-d2ffd73d8df3@acm.org
 */
#ifndef IOURINGINLINE
#if defined(__cplusplus) && __cplusplus >= 202002L
#define IOURINGINLINE inline
#else
#define IOURINGINLINE static inline
#endif
#endif

#ifndef _LOCAL_INLINE
#if defined(__cplusplus) && __cplusplus >= 202002L
#define _LOCAL_INLINE inline
#else
#define _LOCAL_INLINE static inline
#endif
#endif

/*
 * barrier.h needs _LOCAL_INLINE.
 */
#include "liburing/barrier.h"

#ifdef __alpha__
/*
 * alpha and mips are the exceptions, all other architectures have
 * common numbers for new system calls.
 */
#ifndef __NR_io_uring_setup
#define __NR_io_uring_setup		535
#endif
#ifndef __NR_io_uring_enter
#define __NR_io_uring_enter		536
#endif
#ifndef __NR_io_uring_register
#define __NR_io_uring_register		537
#endif
#elif defined __mips__
#ifndef __NR_io_uring_setup
#define __NR_io_uring_setup		(__NR_Linux + 425)
#endif
#ifndef __NR_io_uring_enter
#define __NR_io_uring_enter		(__NR_Linux + 426)
#endif
#ifndef __NR_io_uring_register
#define __NR_io_uring_register		(__NR_Linux + 427)
#endif
#else /* !__alpha__ and !__mips__ */
#ifndef __NR_io_uring_setup
#define __NR_io_uring_setup		425
#endif
#ifndef __NR_io_uring_enter
#define __NR_io_uring_enter		426
#endif
#ifndef __NR_io_uring_register
#define __NR_io_uring_register		427
#endif
#endif

#ifdef __cplusplus
#define LIBURING_NOEXCEPT noexcept
#else
#define LIBURING_NOEXCEPT
#endif

#ifdef __cplusplus
extern "C" {
#endif

/*
 * Library interface to io_uring
 */
struct io_uring_sq {
	unsigned *khead;
	unsigned *ktail;
	// Deprecated: use `ring_mask` instead of `*kring_mask`
	unsigned *kring_mask;
	// Deprecated: use `ring_entries` instead of `*kring_entries`
	unsigned *kring_entries;
	unsigned *kflags;
	unsigned *kdropped;
	unsigned *array;
	struct io_uring_sqe *sqes;

	unsigned sqe_head;
	unsigned sqe_tail;

	size_t ring_sz;
	void *ring_ptr;

	unsigned ring_mask;
	unsigned ring_entries;

	unsigned pad[2];
};

struct io_uring_cq {
	unsigned *khead;
	unsigned *ktail;
	// Deprecated: use `ring_mask` instead of `*kring_mask`
	unsigned *kring_mask;
	// Deprecated: use `ring_entries` instead of `*kring_entries`
	unsigned *kring_entries;
	unsigned *kflags;
	unsigned *koverflow;
	struct io_uring_cqe *cqes;

	size_t ring_sz;
	void *ring_ptr;

	unsigned ring_mask;
	unsigned ring_entries;

	unsigned pad[2];
};

struct io_uring {
	struct io_uring_sq sq;
	struct io_uring_cq cq;
	unsigned flags;
	int ring_fd;

	unsigned features;
	int enter_ring_fd;
	__u8 int_flags;
	__u8 pad[3];
	unsigned pad2;
};

struct io_uring_zcrx_rq {
	__u32 *khead;
	__u32 *ktail;
	__u32 rq_tail;
	unsigned ring_entries;

	struct io_uring_zcrx_rqe *rqes;
	void *ring_ptr;
};

/*
 * Library interface
 */

_LOCAL_INLINE __u64 uring_ptr_to_u64(const void *ptr) LIBURING_NOEXCEPT
{
	return (__u64) (unsigned long) ptr;
}

/*
 * return an allocated io_uring_probe structure, or NULL if probe fails (for
 * example, if it is not available). The caller is responsible for freeing it
 */
struct io_uring_probe *io_uring_get_probe_ring(struct io_uring *ring)
	LIBURING_NOEXCEPT;
/* same as io_uring_get_probe_ring, but takes care of ring init and teardown */
struct io_uring_probe *io_uring_get_probe(void) LIBURING_NOEXCEPT;

/*
 * frees a probe allocated through io_uring_get_probe() or
 * io_uring_get_probe_ring()
 */
void io_uring_free_probe(struct io_uring_probe *probe) LIBURING_NOEXCEPT;

IOURINGINLINE int io_uring_opcode_supported(const struct io_uring_probe *p,
					    int op) LIBURING_NOEXCEPT
{
	if (op > p->last_op)
		return 0;
	return (p->ops[op].flags & IO_URING_OP_SUPPORTED) != 0;
}

int io_uring_queue_init_mem(unsigned entries, struct io_uring *ring,
				struct io_uring_params *p,
				void *buf, size_t buf_size) LIBURING_NOEXCEPT;
int io_uring_queue_init_params(unsigned entries, struct io_uring *ring,
				struct io_uring_params *p) LIBURING_NOEXCEPT;
int io_uring_queue_init(unsigned entries, struct io_uring *ring,
			unsigned flags) LIBURING_NOEXCEPT;
int io_uring_queue_mmap(int fd, struct io_uring_params *p,
			struct io_uring *ring) LIBURING_NOEXCEPT;
int io_uring_ring_dontfork(struct io_uring *ring) LIBURING_NOEXCEPT;
void io_uring_queue_exit(struct io_uring *ring) LIBURING_NOEXCEPT;
unsigned io_uring_peek_batch_cqe(struct io_uring *ring,
	struct io_uring_cqe **cqes, unsigned count) LIBURING_NOEXCEPT;
int io_uring_wait_cqes(struct io_uring *ring, struct io_uring_cqe **cqe_ptr,
		       unsigned wait_nr, struct __kernel_timespec *ts,
		       sigset_t *sigmask) LIBURING_NOEXCEPT;
int io_uring_wait_cqes_min_timeout(struct io_uring *ring,
				   struct io_uring_cqe **cqe_ptr,
				   unsigned wait_nr,
				   struct __kernel_timespec *ts,
				   unsigned int min_ts_usec,
				   sigset_t *sigmask) LIBURING_NOEXCEPT;
int io_uring_wait_cqe_timeout(struct io_uring *ring,
			      struct io_uring_cqe **cqe_ptr,
			      struct __kernel_timespec *ts) LIBURING_NOEXCEPT;
int io_uring_submit(struct io_uring *ring) LIBURING_NOEXCEPT;
int io_uring_submit_and_wait(struct io_uring *ring, unsigned wait_nr)
	LIBURING_NOEXCEPT;
int io_uring_submit_and_wait_timeout(struct io_uring *ring,
				     struct io_uring_cqe **cqe_ptr,
				     unsigned wait_nr,
				     struct __kernel_timespec *ts,
				     sigset_t *sigmask) LIBURING_NOEXCEPT;
int io_uring_submit_and_wait_min_timeout(struct io_uring *ring,
					 struct io_uring_cqe **cqe_ptr,
					 unsigned wait_nr,
					 struct __kernel_timespec *ts,
					 unsigned min_wait,
					 sigset_t *sigmask) LIBURING_NOEXCEPT;
int io_uring_submit_and_wait_reg(struct io_uring *ring,
				 struct io_uring_cqe **cqe_ptr, unsigned wait_nr,
				 int reg_index) LIBURING_NOEXCEPT;

int io_uring_register_wait_reg(struct io_uring *ring,
			       struct io_uring_reg_wait *reg, int nr)
   LIBURING_NOEXCEPT;
int io_uring_resize_rings(struct io_uring *ring, struct io_uring_params *p)
	LIBURING_NOEXCEPT;
int io_uring_clone_buffers_offset(struct io_uring *dst, struct io_uring *src,
				  unsigned int dst_off, unsigned int src_off,
				  unsigned int nr, unsigned int flags)
	LIBURING_NOEXCEPT;
int __io_uring_clone_buffers_offset(struct io_uring *dst, struct io_uring *src,
				  unsigned int dst_off, unsigned int src_off,
				  unsigned int nr, unsigned int flags)
	LIBURING_NOEXCEPT;
int io_uring_clone_buffers(struct io_uring *dst, struct io_uring *src)
	LIBURING_NOEXCEPT;
int __io_uring_clone_buffers(struct io_uring *dst, struct io_uring *src,
			     unsigned int flags) LIBURING_NOEXCEPT;
int io_uring_register_buffers(struct io_uring *ring, const struct iovec *iovecs,
			      unsigned nr_iovecs) LIBURING_NOEXCEPT;
int io_uring_register_buffers_tags(struct io_uring *ring,
				   const struct iovec *iovecs,
				   const __u64 *tags, unsigned nr)
	LIBURING_NOEXCEPT;
int io_uring_register_buffers_sparse(struct io_uring *ring, unsigned nr)
	LIBURING_NOEXCEPT;
int io_uring_register_buffers_update_tag(struct io_uring *ring,
					 unsigned off,
					 const struct iovec *iovecs,
					 const __u64 *tags, unsigned nr)
	LIBURING_NOEXCEPT;
int io_uring_unregister_buffers(struct io_uring *ring) LIBURING_NOEXCEPT;

int io_uring_register_files(struct io_uring *ring, const int *files,
			    unsigned nr_files) LIBURING_NOEXCEPT;
int io_uring_register_files_tags(struct io_uring *ring, const int *files,
				 const __u64 *tags, unsigned nr)
	LIBURING_NOEXCEPT;
int io_uring_register_files_sparse(struct io_uring *ring, unsigned nr)
	LIBURING_NOEXCEPT;
int io_uring_register_files_update_tag(struct io_uring *ring, unsigned off,
				       const int *files, const __u64 *tags,
				       unsigned nr_files) LIBURING_NOEXCEPT;

int io_uring_unregister_files(struct io_uring *ring) LIBURING_NOEXCEPT;
int io_uring_register_files_update(struct io_uring *ring, unsigned off,
				   const int *files, unsigned nr_files)
	LIBURING_NOEXCEPT;
int io_uring_register_eventfd(struct io_uring *ring, int fd) LIBURING_NOEXCEPT;
int io_uring_register_eventfd_async(struct io_uring *ring, int fd)
	LIBURING_NOEXCEPT;
int io_uring_unregister_eventfd(struct io_uring *ring) LIBURING_NOEXCEPT;
int io_uring_register_probe(struct io_uring *ring, struct io_uring_probe *p,
			    unsigned nr) LIBURING_NOEXCEPT;
int io_uring_register_personality(struct io_uring *ring) LIBURING_NOEXCEPT;
int io_uring_unregister_personality(struct io_uring *ring, int id)
	LIBURING_NOEXCEPT;
int io_uring_register_restrictions(struct io_uring *ring,
				   struct io_uring_restriction *res,
				   unsigned int nr_res) LIBURING_NOEXCEPT;
int io_uring_enable_rings(struct io_uring *ring) LIBURING_NOEXCEPT;
int __io_uring_sqring_wait(struct io_uring *ring) LIBURING_NOEXCEPT;
#ifdef _GNU_SOURCE
int io_uring_register_iowq_aff(struct io_uring *ring, size_t cpusz,
				const cpu_set_t *mask) LIBURING_NOEXCEPT;
#endif
int io_uring_unregister_iowq_aff(struct io_uring *ring) LIBURING_NOEXCEPT;
int io_uring_register_iowq_max_workers(struct io_uring *ring,
				       unsigned int *values) LIBURING_NOEXCEPT;
int io_uring_register_ring_fd(struct io_uring *ring) LIBURING_NOEXCEPT;
int io_uring_unregister_ring_fd(struct io_uring *ring) LIBURING_NOEXCEPT;
int io_uring_close_ring_fd(struct io_uring *ring) LIBURING_NOEXCEPT;
int io_uring_register_buf_ring(struct io_uring *ring,
	struct io_uring_buf_reg *reg, unsigned int flags) LIBURING_NOEXCEPT;
int io_uring_unregister_buf_ring(struct io_uring *ring, int bgid)
	LIBURING_NOEXCEPT;
int io_uring_buf_ring_head(struct io_uring *ring,
	int buf_group, uint16_t *head) LIBURING_NOEXCEPT;
int io_uring_register_sync_cancel(struct io_uring *ring,
				  struct io_uring_sync_cancel_reg *reg)
	LIBURING_NOEXCEPT;
int io_uring_register_sync_msg(struct io_uring_sqe *sqe) LIBURING_NOEXCEPT;

int io_uring_register_file_alloc_range(struct io_uring *ring,
				       unsigned off, unsigned len)
	LIBURING_NOEXCEPT;

int io_uring_register_napi(struct io_uring *ring, struct io_uring_napi *napi)
	LIBURING_NOEXCEPT;
int io_uring_unregister_napi(struct io_uring *ring, struct io_uring_napi *napi)
	LIBURING_NOEXCEPT;
int io_uring_register_ifq(struct io_uring *ring,
			  struct io_uring_zcrx_ifq_reg *reg) LIBURING_NOEXCEPT;

int io_uring_register_clock(struct io_uring *ring,
			    struct io_uring_clock_register *arg)
   LIBURING_NOEXCEPT;

int io_uring_get_events(struct io_uring *ring) LIBURING_NOEXCEPT;
int io_uring_submit_and_get_events(struct io_uring *ring) LIBURING_NOEXCEPT;

/*
 * io_uring syscalls.
 */
int io_uring_enter(unsigned int fd, unsigned int to_submit,
		   unsigned int min_complete, unsigned int flags, sigset_t *sig)
	LIBURING_NOEXCEPT;
int io_uring_enter2(unsigned int fd, unsigned int to_submit,
		    unsigned int min_complete, unsigned int flags,
		    void *arg, size_t sz) LIBURING_NOEXCEPT;
int io_uring_setup(unsigned int entries, struct io_uring_params *p)
	LIBURING_NOEXCEPT;
int io_uring_register(unsigned int fd, unsigned int opcode, const void *arg,
		      unsigned int nr_args) LIBURING_NOEXCEPT;

/*
 * Mapped/registered regions
 */
int io_uring_register_region(struct io_uring *ring,
			     struct io_uring_mem_region_reg *reg)
	LIBURING_NOEXCEPT;

/*
 * Mapped buffer ring alloc/register + unregister/free helpers
 */
struct io_uring_buf_ring *io_uring_setup_buf_ring(struct io_uring *ring,
						  unsigned int nentries,
						  int bgid, unsigned int flags,
						  int *err) LIBURING_NOEXCEPT;
int io_uring_free_buf_ring(struct io_uring *ring, struct io_uring_buf_ring *br,
			   unsigned int nentries, int bgid) LIBURING_NOEXCEPT;

/*
 * Helper for the peek/wait single cqe functions. Exported because of that,
 * but probably shouldn't be used directly in an application.
 */
int __io_uring_get_cqe(struct io_uring *ring,
			struct io_uring_cqe **cqe_ptr, unsigned submit,
			unsigned wait_nr, sigset_t *sigmask) LIBURING_NOEXCEPT;

/*
 * Enable/disable setting of iowait by the kernel.
 */
int io_uring_set_iowait(struct io_uring *ring, bool enable_iowait)
	LIBURING_NOEXCEPT;

#define LIBURING_UDATA_TIMEOUT	((__u64) -1)

/*
 * Returns the bit shift needed to index the CQ.
 * This shift is 1 for rings with big CQEs, and 0 for rings with normal CQEs.
 * CQE `index` can be computed as &cq.cqes[(index & cq.ring_mask) << cqe_shift].
 */
unsigned io_uring_cqe_shift_from_flags(unsigned flags)
	LIBURING_NOEXCEPT;

unsigned io_uring_cqe_shift(const struct io_uring *ring)
	LIBURING_NOEXCEPT;

unsigned io_uring_cqe_nr(const struct io_uring_cqe *cqe);

struct io_uring_cqe_iter {
	struct io_uring_cqe *cqes;
	unsigned mask;
	unsigned shift;
	unsigned head;
	unsigned tail;
};

_LOCAL_INLINE struct io_uring_cqe_iter
io_uring_cqe_iter_init(const struct io_uring *ring)
	LIBURING_NOEXCEPT
{
	return (struct io_uring_cqe_iter) {
		.cqes = ring->cq.cqes,
		.mask = ring->cq.ring_mask,
		.shift = io_uring_cqe_shift(ring),
		.head = *ring->cq.khead,
		/* Acquire ordering ensures tail is loaded before any CQEs */
		.tail = io_uring_smp_load_acquire(ring->cq.ktail),
	};
}

_LOCAL_INLINE bool io_uring_cqe_iter_next(struct io_uring_cqe_iter *iter,
					  struct io_uring_cqe **cqe)
	LIBURING_NOEXCEPT
{
	if (iter->head == iter->tail)
		return false;

	*cqe = &iter->cqes[(iter->head++ & iter->mask) << iter->shift];
	if ((*cqe)->flags & IORING_CQE_F_32)
		iter->head++;
	return true;
}

/*
 * NOTE: we should just get rid of the '__head__' being passed in here, it doesn't
 * serve a purpose anymore. The below is a bit of a work-around to ensure that
 * the compiler doesn't complain about '__head__' being unused (or only written,
 * never read), as we use a local iterator for both the head and tail tracking.
 */
#define io_uring_for_each_cqe(ring, __head__, cqe)					\
	for (struct io_uring_cqe_iter __ITER__ = io_uring_cqe_iter_init(ring);	\
	     (__head__) = __ITER__.head, io_uring_cqe_iter_next(&__ITER__, &(cqe));	\
	     (void)(__head__))

/*
 * Must be called after io_uring_for_each_cqe()
 */
void io_uring_cq_advance(struct io_uring *ring, unsigned nr)
	LIBURING_NOEXCEPT;

/*
 * Must be called after io_uring_{peek,wait}_cqe() after the cqe has
 * been processed by the application.
 */
void io_uring_cqe_seen(struct io_uring *ring,
				     struct io_uring_cqe *cqe)
	LIBURING_NOEXCEPT;

/*
 * Command prep helpers
 */

/*
 * Associate pointer @data with the sqe, for later retrieval from the cqe
 * at command completion time with io_uring_cqe_get_data().
 */
void io_uring_sqe_set_data(struct io_uring_sqe *sqe, void *data)
	LIBURING_NOEXCEPT;

void *io_uring_cqe_get_data(const struct io_uring_cqe *cqe);

/*
 * Assign a 64-bit value to this sqe, which can get retrieved at completion
 * time with io_uring_cqe_get_data64. Just like the non-64 variants, except
 * these store a 64-bit type rather than a data pointer.
 */
void io_uring_sqe_set_data64(struct io_uring_sqe *sqe,
					   __u64 data)
	LIBURING_NOEXCEPT;

__u64 io_uring_cqe_get_data64(const struct io_uring_cqe *cqe);

/*
 * Tell the app the have the 64-bit variants of the get/set userdata
 */
#define LIBURING_HAVE_DATA64

void io_uring_sqe_set_flags(struct io_uring_sqe *sqe,
					  unsigned flags)
	LIBURING_NOEXCEPT;

void io_uring_sqe_set_buf_group(struct io_uring_sqe *sqe,
					      int bgid)
	LIBURING_NOEXCEPT;

_LOCAL_INLINE void __io_uring_set_target_fixed_file(struct io_uring_sqe *sqe,
						    unsigned int file_index)
	LIBURING_NOEXCEPT
{
	/* 0 means no fixed files, indexes should be encoded as "index + 1" */
	sqe->file_index = file_index + 1;
}

void io_uring_initialize_sqe(struct io_uring_sqe *sqe)
	LIBURING_NOEXCEPT;

void io_uring_prep_rw(int op, struct io_uring_sqe *sqe, int fd,
				    const void *addr, unsigned len,
				    __u64 offset)
	LIBURING_NOEXCEPT;

/*
 * io_uring_prep_splice() - Either @fd_in or @fd_out must be a pipe.
 *
 * - If @fd_in refers to a pipe, @off_in is ignored and must be set to -1.
 *
 * - If @fd_in does not refer to a pipe and @off_in is -1, then @nbytes are read
 *   from @fd_in starting from the file offset, which is incremented by the
 *   number of bytes read.
 *
 * - If @fd_in does not refer to a pipe and @off_in is not -1, then the starting
 *   offset of @fd_in will be @off_in.
 *
 * This splice operation can be used to implement sendfile by splicing to an
 * intermediate pipe first, then splice to the final destination.
 * In fact, the implementation of sendfile in kernel uses splice internally.
 *
 * NOTE that even if fd_in or fd_out refers to a pipe, the splice operation
 * can still fail with EINVAL if one of the fd doesn't explicitly support splice
 * operation, e.g. reading from terminal is unsupported from kernel 5.7 to 5.11.
 * Check issue #291 for more information.
 */
void io_uring_prep_splice(struct io_uring_sqe *sqe,
					int fd_in, int64_t off_in,
					int fd_out, int64_t off_out,
					unsigned int nbytes,
					unsigned int splice_flags)
	LIBURING_NOEXCEPT;

void io_uring_prep_tee(struct io_uring_sqe *sqe,
				     int fd_in, int fd_out,
				     unsigned int nbytes,
				     unsigned int splice_flags)
	LIBURING_NOEXCEPT;

void io_uring_prep_readv(struct io_uring_sqe *sqe, int fd,
				       const struct iovec *iovecs,
				       unsigned nr_vecs, __u64 offset)
	LIBURING_NOEXCEPT;

void io_uring_prep_readv2(struct io_uring_sqe *sqe, int fd,
				       const struct iovec *iovecs,
				       unsigned nr_vecs, __u64 offset,
				       int flags)
	LIBURING_NOEXCEPT;

void io_uring_prep_read_fixed(struct io_uring_sqe *sqe, int fd,
					    void *buf, unsigned nbytes,
					    __u64 offset, int buf_index)
	LIBURING_NOEXCEPT;

void io_uring_prep_readv_fixed(struct io_uring_sqe *sqe, int fd,
					     const struct iovec *iovecs,
					     unsigned nr_vecs, __u64 offset,
					     int flags, int buf_index)
	LIBURING_NOEXCEPT;

void io_uring_prep_writev(struct io_uring_sqe *sqe, int fd,
					const struct iovec *iovecs,
					unsigned nr_vecs, __u64 offset)
	LIBURING_NOEXCEPT;

void io_uring_prep_writev2(struct io_uring_sqe *sqe, int fd,
				       const struct iovec *iovecs,
				       unsigned nr_vecs, __u64 offset,
			       int flags)
	LIBURING_NOEXCEPT;

void io_uring_prep_write_fixed(struct io_uring_sqe *sqe, int fd,
					     const void *buf, unsigned nbytes,
					     __u64 offset, int buf_index)
	LIBURING_NOEXCEPT;

void io_uring_prep_writev_fixed(struct io_uring_sqe *sqe, int fd,
				       const struct iovec *iovecs,
				       unsigned nr_vecs, __u64 offset,
				       int flags, int buf_index)
	LIBURING_NOEXCEPT;

void io_uring_prep_recvmsg(struct io_uring_sqe *sqe, int fd,
					 struct msghdr *msg, unsigned flags)
	LIBURING_NOEXCEPT;

void io_uring_prep_recvmsg_multishot(struct io_uring_sqe *sqe,
						   int fd, struct msghdr *msg,
						   unsigned flags)
	LIBURING_NOEXCEPT;

void io_uring_prep_sendmsg(struct io_uring_sqe *sqe, int fd,
					 const struct msghdr *msg,
					 unsigned flags)
	LIBURING_NOEXCEPT;

_LOCAL_INLINE unsigned __io_uring_prep_poll_mask(unsigned poll_mask)
	LIBURING_NOEXCEPT
{
#if __BYTE_ORDER == __BIG_ENDIAN
	poll_mask = __swahw32(poll_mask);
#endif
	return poll_mask;
}

void io_uring_prep_poll_add(struct io_uring_sqe *sqe, int fd,
					  unsigned poll_mask)
	LIBURING_NOEXCEPT;

void io_uring_prep_poll_multishot(struct io_uring_sqe *sqe,
						int fd, unsigned poll_mask)
	LIBURING_NOEXCEPT;

void io_uring_prep_poll_remove(struct io_uring_sqe *sqe,
					     __u64 user_data)
	LIBURING_NOEXCEPT;

void io_uring_prep_poll_update(struct io_uring_sqe *sqe,
					     __u64 old_user_data,
					     __u64 new_user_data,
					     unsigned poll_mask, unsigned flags)
	LIBURING_NOEXCEPT;

void io_uring_prep_fsync(struct io_uring_sqe *sqe, int fd,
				       unsigned fsync_flags)
	LIBURING_NOEXCEPT;

void io_uring_prep_nop(struct io_uring_sqe *sqe)
	LIBURING_NOEXCEPT;

void io_uring_prep_timeout(struct io_uring_sqe *sqe,
					 struct __kernel_timespec *ts,
					 unsigned count, unsigned flags)
	LIBURING_NOEXCEPT;

void io_uring_prep_timeout_remove(struct io_uring_sqe *sqe,
						__u64 user_data, unsigned flags)
	LIBURING_NOEXCEPT;

void io_uring_prep_timeout_update(struct io_uring_sqe *sqe,
						struct __kernel_timespec *ts,
						__u64 user_data, unsigned flags)
	LIBURING_NOEXCEPT;

void io_uring_prep_accept(struct io_uring_sqe *sqe, int fd,
					struct sockaddr *addr,
					socklen_t *addrlen, int flags)
	LIBURING_NOEXCEPT;

/* accept directly into the fixed file table */
void io_uring_prep_accept_direct(struct io_uring_sqe *sqe, int fd,
					       struct sockaddr *addr,
					       socklen_t *addrlen, int flags,
					       unsigned int file_index)
	LIBURING_NOEXCEPT;

void io_uring_prep_multishot_accept(struct io_uring_sqe *sqe,
						  int fd, struct sockaddr *addr,
						  socklen_t *addrlen, int flags)
	LIBURING_NOEXCEPT;

/* multishot accept directly into the fixed file table */
void io_uring_prep_multishot_accept_direct(struct io_uring_sqe *sqe,
							 int fd,
							 struct sockaddr *addr,
							 socklen_t *addrlen,
							 int flags)
	LIBURING_NOEXCEPT;

void io_uring_prep_cancel64(struct io_uring_sqe *sqe,
					  __u64 user_data, int flags)
	LIBURING_NOEXCEPT;

void io_uring_prep_cancel(struct io_uring_sqe *sqe,
					void *user_data, int flags)
	LIBURING_NOEXCEPT;

void io_uring_prep_cancel_fd(struct io_uring_sqe *sqe, int fd,
					   unsigned int flags)
	LIBURING_NOEXCEPT;

void io_uring_prep_link_timeout(struct io_uring_sqe *sqe,
					      struct __kernel_timespec *ts,
					      unsigned flags)
	LIBURING_NOEXCEPT;

void io_uring_prep_connect(struct io_uring_sqe *sqe, int fd,
					 const struct sockaddr *addr,
					 socklen_t addrlen)
	LIBURING_NOEXCEPT;

void io_uring_prep_bind(struct io_uring_sqe *sqe, int fd,
				      struct sockaddr *addr,
				      socklen_t addrlen)
	LIBURING_NOEXCEPT;

void io_uring_prep_listen(struct io_uring_sqe *sqe, int fd,
				      int backlog)
	LIBURING_NOEXCEPT;

struct epoll_event;
void io_uring_prep_epoll_wait(struct io_uring_sqe *sqe, int fd,
					    struct epoll_event *events,
					    int maxevents, unsigned flags)
	LIBURING_NOEXCEPT;

void io_uring_prep_files_update(struct io_uring_sqe *sqe,
					      int *fds, unsigned nr_fds,
					      int offset)
	LIBURING_NOEXCEPT;

void io_uring_prep_fallocate(struct io_uring_sqe *sqe, int fd,
					   int mode, __u64 offset, __u64 len)
	LIBURING_NOEXCEPT;

void io_uring_prep_openat(struct io_uring_sqe *sqe, int dfd,
					const char *path, int flags,
					mode_t mode)
	LIBURING_NOEXCEPT;

/* open directly into the fixed file table */
void io_uring_prep_openat_direct(struct io_uring_sqe *sqe,
					       int dfd, const char *path,
					       int flags, mode_t mode,
					       unsigned file_index)
	LIBURING_NOEXCEPT;

void io_uring_prep_open(struct io_uring_sqe *sqe,
					const char *path, int flags, mode_t mode)
	LIBURING_NOEXCEPT;

/* open directly into the fixed file table */
void io_uring_prep_open_direct(struct io_uring_sqe *sqe,
							const char *path, int flags, mode_t mode,
							unsigned file_index)
	LIBURING_NOEXCEPT;

void io_uring_prep_close(struct io_uring_sqe *sqe, int fd)
	LIBURING_NOEXCEPT;

void io_uring_prep_close_direct(struct io_uring_sqe *sqe,
					      unsigned file_index)
	LIBURING_NOEXCEPT;

void io_uring_prep_read(struct io_uring_sqe *sqe, int fd,
				      void *buf, unsigned nbytes, __u64 offset)
	LIBURING_NOEXCEPT;

void io_uring_prep_read_multishot(struct io_uring_sqe *sqe,
						int fd, unsigned nbytes,
						__u64 offset, int buf_group)
	LIBURING_NOEXCEPT;

void io_uring_prep_write(struct io_uring_sqe *sqe, int fd,
				       const void *buf, unsigned nbytes,
				       __u64 offset)
	LIBURING_NOEXCEPT;

struct statx;
void io_uring_prep_statx(struct io_uring_sqe *sqe, int dfd,
				       const char *path, int flags,
				       unsigned mask, struct statx *statxbuf)
	LIBURING_NOEXCEPT;

void io_uring_prep_fadvise(struct io_uring_sqe *sqe, int fd,
					 __u64 offset, __u32 len, int advice)
	LIBURING_NOEXCEPT;

void io_uring_prep_madvise(struct io_uring_sqe *sqe, void *addr,
					 __u32 length, int advice)
	LIBURING_NOEXCEPT;

void io_uring_prep_fadvise64(struct io_uring_sqe *sqe, int fd,
					 __u64 offset, off_t len, int advice)
	LIBURING_NOEXCEPT;

void io_uring_prep_madvise64(struct io_uring_sqe *sqe, void *addr,
					 off_t length, int advice)
	LIBURING_NOEXCEPT;

void io_uring_prep_send(struct io_uring_sqe *sqe, int sockfd,
				      const void *buf, size_t len, int flags)
	LIBURING_NOEXCEPT;

void io_uring_prep_send_bundle(struct io_uring_sqe *sqe,
					     int sockfd, size_t len, int flags)
	LIBURING_NOEXCEPT;

void io_uring_prep_send_set_addr(struct io_uring_sqe *sqe,
						const struct sockaddr *dest_addr,
						__u16 addr_len)
	LIBURING_NOEXCEPT;

void io_uring_prep_sendto(struct io_uring_sqe *sqe, int sockfd,
					const void *buf, size_t len, int flags,
					const struct sockaddr *addr,
					socklen_t addrlen)
	LIBURING_NOEXCEPT;

void io_uring_prep_send_zc(struct io_uring_sqe *sqe, int sockfd,
					 const void *buf, size_t len, int flags,
					 unsigned zc_flags)
	LIBURING_NOEXCEPT;

void io_uring_prep_send_zc_fixed(struct io_uring_sqe *sqe,
						int sockfd, const void *buf,
						size_t len, int flags,
						unsigned zc_flags,
						unsigned buf_index)
	LIBURING_NOEXCEPT;

void io_uring_prep_sendmsg_zc(struct io_uring_sqe *sqe, int fd,
					    const struct msghdr *msg,
					    unsigned flags)
	LIBURING_NOEXCEPT;

void io_uring_prep_sendmsg_zc_fixed(struct io_uring_sqe *sqe,
						int fd,
						const struct msghdr *msg,
						unsigned flags,
						unsigned buf_index)
	LIBURING_NOEXCEPT;

void io_uring_prep_recv(struct io_uring_sqe *sqe, int sockfd,
				      void *buf, size_t len, int flags)
	LIBURING_NOEXCEPT;

void io_uring_prep_recv_multishot(struct io_uring_sqe *sqe,
						int sockfd, void *buf,
						size_t len, int flags)
	LIBURING_NOEXCEPT;

struct io_uring_recvmsg_out *
io_uring_recvmsg_validate(void *buf, int buf_len, struct msghdr *msgh)
	LIBURING_NOEXCEPT;

void *io_uring_recvmsg_name(struct io_uring_recvmsg_out *o)
	LIBURING_NOEXCEPT;

struct cmsghdr *
io_uring_recvmsg_cmsg_firsthdr(struct io_uring_recvmsg_out *o,
			       struct msghdr *msgh)
	LIBURING_NOEXCEPT;

struct cmsghdr *
io_uring_recvmsg_cmsg_nexthdr(struct io_uring_recvmsg_out *o, struct msghdr *msgh,
			      struct cmsghdr *cmsg)
	LIBURING_NOEXCEPT;

void *io_uring_recvmsg_payload(struct io_uring_recvmsg_out *o,
					     struct msghdr *msgh)
	LIBURING_NOEXCEPT;

unsigned int
io_uring_recvmsg_payload_length(struct io_uring_recvmsg_out *o,
				int buf_len, struct msghdr *msgh)
	LIBURING_NOEXCEPT;

void io_uring_prep_openat2(struct io_uring_sqe *sqe, int dfd,
					const char *path, struct open_how *how)
	LIBURING_NOEXCEPT;

/* open directly into the fixed file table */
void io_uring_prep_openat2_direct(struct io_uring_sqe *sqe,
						int dfd, const char *path,
						struct open_how *how,
						unsigned file_index)
	LIBURING_NOEXCEPT;

struct epoll_event;
void io_uring_prep_epoll_ctl(struct io_uring_sqe *sqe, int epfd,
					   int fd, int op,
					   struct epoll_event *ev)
	LIBURING_NOEXCEPT;

void io_uring_prep_provide_buffers(struct io_uring_sqe *sqe,
						 void *addr, int len, int nr,
						 int bgid, int bid)
	LIBURING_NOEXCEPT;

void io_uring_prep_remove_buffers(struct io_uring_sqe *sqe,
						int nr, int bgid)
	LIBURING_NOEXCEPT;

void io_uring_prep_shutdown(struct io_uring_sqe *sqe, int fd,
					  int how)
	LIBURING_NOEXCEPT;

void io_uring_prep_unlinkat(struct io_uring_sqe *sqe, int dfd,
					  const char *path, int flags)
	LIBURING_NOEXCEPT;

void io_uring_prep_unlink(struct io_uring_sqe *sqe,
					  const char *path, int flags)
	LIBURING_NOEXCEPT;

void io_uring_prep_renameat(struct io_uring_sqe *sqe, int olddfd,
					  const char *oldpath, int newdfd,
					  const char *newpath, unsigned int flags)
	LIBURING_NOEXCEPT;

void io_uring_prep_rename(struct io_uring_sqe *sqe,
					const char *oldpath,
					const char *newpath)
	LIBURING_NOEXCEPT;

void io_uring_prep_sync_file_range(struct io_uring_sqe *sqe,
						 int fd, unsigned len,
						 __u64 offset, int flags)
	LIBURING_NOEXCEPT;

void io_uring_prep_mkdirat(struct io_uring_sqe *sqe, int dfd,
					const char *path, mode_t mode)
	LIBURING_NOEXCEPT;

void io_uring_prep_mkdir(struct io_uring_sqe *sqe,
					const char *path, mode_t mode)
	LIBURING_NOEXCEPT;

void io_uring_prep_symlinkat(struct io_uring_sqe *sqe,
					   const char *target, int newdirfd,
					   const char *linkpath)
	LIBURING_NOEXCEPT;

void io_uring_prep_symlink(struct io_uring_sqe *sqe,
					 const char *target,
					 const char *linkpath)
	LIBURING_NOEXCEPT;

void io_uring_prep_linkat(struct io_uring_sqe *sqe, int olddfd,
					const char *oldpath, int newdfd,
					const char *newpath, int flags)
	LIBURING_NOEXCEPT;

void io_uring_prep_link(struct io_uring_sqe *sqe,
				      const char *oldpath, const char *newpath,
				      int flags)
	LIBURING_NOEXCEPT;

void io_uring_prep_msg_ring_cqe_flags(struct io_uring_sqe *sqe,
					  int fd, unsigned int len, __u64 data,
					  unsigned int flags, unsigned int cqe_flags)
	LIBURING_NOEXCEPT;

void io_uring_prep_msg_ring(struct io_uring_sqe *sqe, int fd,
					  unsigned int len, __u64 data,
					  unsigned int flags)
	LIBURING_NOEXCEPT;

void io_uring_prep_msg_ring_fd(struct io_uring_sqe *sqe, int fd,
					     int source_fd, int target_fd,
					     __u64 data, unsigned int flags)
	LIBURING_NOEXCEPT;

void io_uring_prep_msg_ring_fd_alloc(struct io_uring_sqe *sqe,
						   int fd, int source_fd,
						   __u64 data, unsigned int flags)
	LIBURING_NOEXCEPT;

void io_uring_prep_getxattr(struct io_uring_sqe *sqe,
					  const char *name, char *value,
					  const char *path, unsigned int len)
	LIBURING_NOEXCEPT;

void io_uring_prep_setxattr(struct io_uring_sqe *sqe,
					  const char *name, const char *value,
					  const char *path, int flags,
					  unsigned int len)
	LIBURING_NOEXCEPT;

void io_uring_prep_fgetxattr(struct io_uring_sqe *sqe,
					   int fd, const char *name,
					   char *value, unsigned int len)
	LIBURING_NOEXCEPT;

void io_uring_prep_fsetxattr(struct io_uring_sqe *sqe, int fd,
					   const char *name, const char	*value,
					   int flags, unsigned int len)
	LIBURING_NOEXCEPT;

void io_uring_prep_socket(struct io_uring_sqe *sqe, int domain,
					int type, int protocol,
					unsigned int flags)
	LIBURING_NOEXCEPT;

void io_uring_prep_socket_direct(struct io_uring_sqe *sqe,
					       int domain, int type,
					       int protocol,
					       unsigned file_index,
					       unsigned int flags)
	LIBURING_NOEXCEPT;

void io_uring_prep_socket_direct_alloc(struct io_uring_sqe *sqe,
						     int domain, int type,
						     int protocol,
						     unsigned int flags)
	LIBURING_NOEXCEPT;

/*
 * Prepare commands for sockets
 */
void io_uring_prep_cmd_sock(struct io_uring_sqe *sqe,
					  int cmd_op,
					  int fd,
					  int level,
					  int optname,
					  void *optval,
					  int optlen)
	LIBURING_NOEXCEPT;

void io_uring_prep_waitid(struct io_uring_sqe *sqe,
					idtype_t idtype,
					id_t id,
					siginfo_t *infop,
					int options, unsigned int flags)
	LIBURING_NOEXCEPT;

void io_uring_prep_futex_wake(struct io_uring_sqe *sqe,
					    uint32_t *futex, uint64_t val,
					    uint64_t mask, uint32_t futex_flags,
					    unsigned int flags)
	LIBURING_NOEXCEPT;

void io_uring_prep_futex_wait(struct io_uring_sqe *sqe,
					    uint32_t *futex, uint64_t val,
					    uint64_t mask, uint32_t futex_flags,
					    unsigned int flags)
	LIBURING_NOEXCEPT;

struct futex_waitv;
void io_uring_prep_futex_waitv(struct io_uring_sqe *sqe,
					     struct futex_waitv *futex,
					     uint32_t nr_futex,
					     unsigned int flags)
	LIBURING_NOEXCEPT;

void io_uring_prep_fixed_fd_install(struct io_uring_sqe *sqe,
						  int fd,
						  unsigned int flags)
	LIBURING_NOEXCEPT;

#ifdef _GNU_SOURCE
void io_uring_prep_ftruncate(struct io_uring_sqe *sqe,
				       int fd, loff_t len)
	LIBURING_NOEXCEPT;
#endif

void io_uring_prep_cmd_discard(struct io_uring_sqe *sqe,
					     int fd,
					     uint64_t offset, uint64_t nbytes)
	LIBURING_NOEXCEPT;

void io_uring_prep_pipe(struct io_uring_sqe *sqe, int *fds,
				      int pipe_flags);

/* setup pipe directly into the fixed file table */
void io_uring_prep_pipe_direct(struct io_uring_sqe *sqe, int *fds,
					     int pipe_flags,
					     unsigned int file_index);

/* Read the kernel's SQ head index with appropriate memory ordering */
unsigned io_uring_load_sq_head(const struct io_uring *ring)
	LIBURING_NOEXCEPT;

/*
 * Returns number of unconsumed (if SQPOLL) or unsubmitted entries exist in
 * the SQ ring
 */
unsigned io_uring_sq_ready(const struct io_uring *ring)
	LIBURING_NOEXCEPT;

/*
 * Returns how much space is left in the SQ ring.
 */
unsigned io_uring_sq_space_left(const struct io_uring *ring)
	LIBURING_NOEXCEPT;

/*
 * Returns the bit shift needed to index the SQ.
 * This shift is 1 for rings with big SQEs, and 0 for rings with normal SQEs.
 * SQE `index` can be computed as &sq.sqes[(index & sq.ring_mask) << sqe_shift].
 */
unsigned io_uring_sqe_shift_from_flags(unsigned flags)
	LIBURING_NOEXCEPT;

unsigned io_uring_sqe_shift(const struct io_uring *ring)
	LIBURING_NOEXCEPT;

/*
 * Only applicable when using SQPOLL - allows the caller to wait for space
 * to free up in the SQ ring, which happens when the kernel side thread has
 * consumed one or more entries. If the SQ ring is currently non-full, no
 * action is taken. Note: may return -EINVAL if the kernel doesn't support
 * this feature.
 */
int io_uring_sqring_wait(struct io_uring *ring)
	LIBURING_NOEXCEPT;

/*
 * Returns how many unconsumed entries are ready in the CQ ring
 */
unsigned io_uring_cq_ready(const struct io_uring *ring)
	LIBURING_NOEXCEPT;

/*
 * Returns true if there are overflow entries waiting to be flushed onto
 * the CQ ring
 */
bool io_uring_cq_has_overflow(const struct io_uring *ring)
	LIBURING_NOEXCEPT;

/*
 * Returns true if the eventfd notification is currently enabled
 */
bool io_uring_cq_eventfd_enabled(const struct io_uring *ring)
	LIBURING_NOEXCEPT;

/*
 * Toggle eventfd notification on or off, if an eventfd is registered with
 * the ring.
 */
int io_uring_cq_eventfd_toggle(struct io_uring *ring,
					     bool enabled)
	LIBURING_NOEXCEPT;

/*
 * Return an IO completion, waiting for 'wait_nr' completions if one isn't
 * readily available. Returns 0 with cqe_ptr filled in on success, -errno on
 * failure.
 */
int io_uring_wait_cqe_nr(struct io_uring *ring,
				      struct io_uring_cqe **cqe_ptr,
				      unsigned wait_nr)
	LIBURING_NOEXCEPT;

static inline bool io_uring_skip_cqe(struct io_uring *ring,
				     struct io_uring_cqe *cqe, int *err)
{
	if (cqe->flags & IORING_CQE_F_SKIP)
		goto out;
	if (ring->features & IORING_FEAT_EXT_ARG)
		return false;
	if (cqe->user_data != LIBURING_UDATA_TIMEOUT)
		return false;
	if (cqe->res < 0)
		*err = cqe->res;
out:
	io_uring_cq_advance(ring, io_uring_cqe_nr(cqe));
	return !*err;
}

/*
 * Internal helper, don't use directly in applications. Use one of the
 * "official" versions of this, io_uring_peek_cqe(), io_uring_wait_cqe(),
 * or io_uring_wait_cqes*().
 */
_LOCAL_INLINE int __io_uring_peek_cqe(struct io_uring *ring,
				      struct io_uring_cqe **cqe_ptr,
				      unsigned *nr_available)
	LIBURING_NOEXCEPT
{
	struct io_uring_cqe *cqe;
	int err = 0;
	unsigned available;
	unsigned mask = ring->cq.ring_mask;
	unsigned shift = io_uring_cqe_shift(ring);

	do {
		unsigned tail = io_uring_smp_load_acquire(ring->cq.ktail);

		/**
		 * A load_acquire on the head prevents reordering with the
		 * cqe load below, ensuring that we see the correct cq entry.
		 */
		unsigned head = io_uring_smp_load_acquire(ring->cq.khead);

		cqe = NULL;
		available = tail - head;
		if (!available)
			break;

		cqe = &ring->cq.cqes[(head & mask) << shift];
		if (!io_uring_skip_cqe(ring, cqe, &err))
			break;
		cqe = NULL;
	} while (1);

	*cqe_ptr = cqe;
	if (nr_available)
		*nr_available = available;
	return err;
}

/*
 * Return an IO completion, if one is readily available. Returns 0 with
 * cqe_ptr filled in on success, -errno on failure.
 */
int io_uring_peek_cqe(struct io_uring *ring,
				    struct io_uring_cqe **cqe_ptr)
	LIBURING_NOEXCEPT;

/*
 * Return an IO completion, waiting for it if necessary. Returns 0 with
 * cqe_ptr filled in on success, -errno on failure.
 */
int io_uring_wait_cqe(struct io_uring *ring,
				    struct io_uring_cqe **cqe_ptr)
	LIBURING_NOEXCEPT;

/*
 * Return an sqe to fill. Application must later call io_uring_submit()
 * when it's ready to tell the kernel about it. The caller may call this
 * function multiple times before calling io_uring_submit().
 *
 * Returns a vacant sqe, or NULL if we're full.
 */
struct io_uring_sqe *_io_uring_get_sqe(struct io_uring *ring)
	LIBURING_NOEXCEPT;

/*
 * Return the appropriate mask for a buffer ring of size 'ring_entries'
 */
int io_uring_buf_ring_mask(__u32 ring_entries)
	LIBURING_NOEXCEPT;

void io_uring_buf_ring_init(struct io_uring_buf_ring *br)
	LIBURING_NOEXCEPT;

/*
 * Assign 'buf' with the addr/len/buffer ID supplied
 */
void io_uring_buf_ring_add(struct io_uring_buf_ring *br,
					 void *addr, unsigned int len,
					 unsigned short bid, int mask,
					 int buf_offset)
	LIBURING_NOEXCEPT;

/*
 * Make 'count' new buffers visible to the kernel. Called after
 * io_uring_buf_ring_add() has been called 'count' times to fill in new
 * buffers.
 */
void io_uring_buf_ring_advance(struct io_uring_buf_ring *br,
					     int count)
	LIBURING_NOEXCEPT;

IOURINGINLINE void __io_uring_buf_ring_cq_advance(struct io_uring *ring,
						  struct io_uring_buf_ring *br,
						  int cq_count, int buf_count)
	LIBURING_NOEXCEPT
{
	io_uring_buf_ring_advance(br, buf_count);
	io_uring_cq_advance(ring, cq_count);
}

/*
 * Make 'count' new buffers visible to the kernel while at the same time
 * advancing the CQ ring seen entries. This can be used when the application
 * is using ring provided buffers and returns buffers while processing CQEs,
 * avoiding an extra atomic when needing to increment both the CQ ring and
 * the ring buffer index at the same time.
 */
void io_uring_buf_ring_cq_advance(struct io_uring *ring,
						struct io_uring_buf_ring *br,
						int count)
	LIBURING_NOEXCEPT;

int io_uring_buf_ring_available(struct io_uring *ring,
					      struct io_uring_buf_ring *br,
					      unsigned short bgid)
	LIBURING_NOEXCEPT;

/*
 * As of liburing-2.2, io_uring_get_sqe() has been converted into a
 * "static inline" function. However, this change breaks seamless
 * updates of liburing.so, as applications would need to be recompiled.
 * To ensure backward compatibility, liburing keeps the original
 * io_uring_get_sqe() symbol available in the shared library.
 *
 * To accomplish this, io_uring_get_sqe() is defined as a non-static
 * inline function when LIBURING_INTERNAL is set, which only applies
 * during liburing.so builds.
 *
 * This strategy ensures new users adopt the "static inline" version
 * while preserving compatibility for old applications linked against
 * the shared library.
 *
 * Relevant commits:
 * 8be8af4afcb4 ("queue: provide io_uring_get_sqe() symbol again")
 * 52dcdbba35c8 ("src/queue: protect io_uring_get_sqe() with LIBURING_INTERNAL")
 */
#ifndef LIBURING_INTERNAL
struct io_uring_sqe *io_uring_get_sqe(struct io_uring *ring)
	LIBURING_NOEXCEPT;
#else
struct io_uring_sqe *io_uring_get_sqe(struct io_uring *ring);
#endif

ssize_t io_uring_mlock_size(unsigned entries, unsigned flags)
	LIBURING_NOEXCEPT;
ssize_t io_uring_mlock_size_params(unsigned entries, struct io_uring_params *p)
	LIBURING_NOEXCEPT;

ssize_t io_uring_memory_size(unsigned entries, unsigned flags)
	LIBURING_NOEXCEPT;
ssize_t io_uring_memory_size_params(unsigned entries, struct io_uring_params *p)
	LIBURING_NOEXCEPT;

/*
 * Versioning information for liburing.
 *
 * Use IO_URING_CHECK_VERSION() for compile time checks including from
 * preprocessor directives.
 *
 * Use io_uring_check_version() for runtime checks of the version of
 * liburing that was loaded by the dynamic linker.
 */
int io_uring_major_version(void) LIBURING_NOEXCEPT;
int io_uring_minor_version(void) LIBURING_NOEXCEPT;
bool io_uring_check_version(int major, int minor) LIBURING_NOEXCEPT;

#define IO_URING_CHECK_VERSION(major,minor) \
  (major > IO_URING_VERSION_MAJOR ||        \
   (major == IO_URING_VERSION_MAJOR &&      \
    minor > IO_URING_VERSION_MINOR))

#ifdef __cplusplus
}
#endif

#ifdef IOURINGINLINE
#undef IOURINGINLINE
#endif

#ifdef _LOCAL_INLINE
#undef _LOCAL_INLINE
#endif

#endif
